{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from Trainer import Trainer\n",
    "from preprocessing import dataPreprocessor\n",
    "from parameters import *\n",
    "from RoBERTaModel import *\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import horovod.tensorflow as hvd\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "        hvd.init()\n",
    "        gpus = tf.config.list_physical_devices('GPU') \n",
    "        for gpu in gpus:\n",
    "            print(gpu)\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        if gpus:\n",
    "            print(\"gpus \", gpus)\n",
    "            print(\"local rank \",hvd.local_rank())\n",
    "            tf.config.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "            print(tf.config.get_visible_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"google-quest-challenge/\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
    "submit_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\n",
    "stack_df = pd.read_csv(os.path.join(data_dir, \"stackexchange_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = train_df[train_columns + target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "dataPreprocessor.logger = False\n",
    "dataPreprocessor.tokenizer = tokenizer\n",
    "dataPreprocessor.model = \"Roberta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving tokenizer in  ./checkpoints/RoBERTaForQALabeling_tokenizer_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./checkpoints/RoBERTaForQALabeling_tokenizer_data/vocab.json',\n",
       " './checkpoints/RoBERTaForQALabeling_tokenizer_data/merges.txt',\n",
       " './checkpoints/RoBERTaForQALabeling_tokenizer_data/special_tokens_map.json',\n",
       " './checkpoints/RoBERTaForQALabeling_tokenizer_data/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"RoBERTaForQALabeling\"\n",
    "\n",
    "checkpoint_dir = os.path.join(save_dir, \"{}_tokenizer_data\" .format(model_name))\n",
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(\"saving tokenizer in \", checkpoint_dir)\n",
    "tokenizer.save_pretrained(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1/5 \n",
      "Preprocessing train data\n",
      "Preprocessing test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e82c94ea4ff483fa868c9a96c4d9870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=657434796.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-33f3aa9bf593>\", line 3, in <module>\n",
      "    input_df=input_df[:30])\n",
      "  File \"/shared/ML/KaggleQA_Labeling/Trainer.py\", line 84, in train\n",
      "    model = models[model_name][0].from_pretrained(weights, num_labels=num_labels, output_hidden_states=True)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_utils.py\", line 314, in from_pretrained\n",
      "    proxies=proxies,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\", line 254, in cached_path\n",
      "    local_files_only=local_files_only,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\", line 487, in get_from_cache\n",
      "    http_get(url, temp_file, proxies=proxies, resume_size=resume_size, user_agent=user_agent)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\", line 375, in http_get\n",
      "    for chunk in response.iter_content(chunk_size=1024):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/requests/models.py\", line 750, in generate\n",
      "    for chunk in self.raw.stream(chunk_size, decode_content=True):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/response.py\", line 564, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/response.py\", line 500, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/usr/lib/python3.6/contextlib.py\", line 159, in helper\n",
      "    return _GeneratorContextManager(func, args, kwds)\n",
      "  File \"/usr/lib/python3.6/contextlib.py\", line 59, in __init__\n",
      "    def __init__(self, func, args, kwds):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "Trainer.train(model_name=model_name,\n",
    "              tokenizer=tokenizer,\n",
    "              input_df=input_df[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.pseudo_predict(model_name=model_name, \n",
    "                       pseudo_df=stack_df[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS  0\n",
      "SEP  2\n",
      "PAD  1\n",
      "MASK 50264\n",
      "[40520]\n",
      "Fight\n"
     ]
    }
   ],
   "source": [
    "print(\"CLS \", tokenizer.cls_token_id)\n",
    "print(\"SEP \", tokenizer.sep_token_id)\n",
    "print(\"PAD \", tokenizer.pad_token_id)\n",
    "print(\"MASK\", tokenizer.mask_token_id)\n",
    "\n",
    "inputs = [0, 3,4,5,65,76,787,2 ,98,856,34,0, 543,654,15, 245, 5678, 435, 1, 1, 1, 1]\n",
    "\n",
    "vocab_without_tokens = []\n",
    "\n",
    "aaa = tokenizer.encode(\"Fight\", add_special_tokens=False)\n",
    "print(aaa)\n",
    "print(tokenizer.decode(aaa))\n",
    "\n",
    "tokenizer_vocab = tokenizer.get_vocab().keys() - tokenizer.special_tokens_map.values()\n",
    "\n",
    "for key in tokenizer_vocab:\n",
    "    if key != tokenizer.cls_token and key != tokenizer.sep_token and key != tokenizer.pad_token:\n",
    "        vocab_without_tokens.append(key)\n",
    "\n",
    "        \n",
    "def mask_tokens(inputs, tokenizer, tokenizer_vocab):        \n",
    "\n",
    "    feasible_indexes = []\n",
    "    \n",
    "    # not masking special tokens\n",
    "    for idx, token in enumerate(inputs):\n",
    "        if token != tokenizer.cls_token_id and token != tokenizer.sep_token_id and token != tokenizer.pad_token_id:\n",
    "            feasible_indexes.append(idx)\n",
    "\n",
    "    \n",
    "    masked_input = copy.copy(inputs)\n",
    "    \n",
    "    #shuffle randmly and get 15% of data\n",
    "\n",
    "    np.random.shuffle(feasible_indexes)\n",
    "    desired_change_count = max(1, int(len(feasible_indexes)*0.15))\n",
    "\n",
    "    changed_amount = 0\n",
    "\n",
    "    for idx in feasible_indexes:\n",
    "        # if we changed desired amount of tokens break\n",
    "        if changed_amount == desired_change_count:\n",
    "            break\n",
    "\n",
    "        # 80% to mask token\n",
    "        if np.random.random() < 0.8:\n",
    "            masked_input[idx] = tokenizer.mask_token_id\n",
    "            changed_amount += 1\n",
    "            continue\n",
    "\n",
    "        # 10% to change it with random word from vocabulary\n",
    "        # because ther eare only 20% left we have two options:\n",
    "        # random word\n",
    "        # leave it a it was\n",
    "        if np.random.random() < 0.5:\n",
    "            masked_input[idx] = np.random.randint(0, len(tokenizer_vocab))\n",
    "            changed_amount += 1\n",
    "            continue\n",
    "        # else 10% chance to leave it as it was\n",
    "        else:\n",
    "            masked_input[idx] = masked_input[idx]\n",
    "            continue\n",
    "\n",
    "    return masked_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mask_tokens(inputs, tokenizer, vocab_without_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 4, 5, 65, 76, 787, 2, 98, 856, 34, 0, 50264, 654, 13898, 245, 5678, 435, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['<s>', '</s>', '<unk>', '</s>', '<pad>', '<s>', '<mask>'])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable([[1,2,3,4,5,6,76,8]], dtype=tf.int64)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 1  2  3 54  5  6 76  8]], shape=(1, 8), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "b = tf.concat([a[:, :3],[[54]],a[:, 3+1:]], axis = -1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
